{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29fb9782-723b-43fc-9df3-c2488ec55dce",
   "metadata": {},
   "source": [
    "## Convert .dat file to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e02fc64-5321-490d-ad70-898e00ac0ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of measurements: 253988\n",
      "test_measurements/event000000001-measurements.dat saved to test_measurements/measurements.csv\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_binary_data(binary_file, start_offset, ele_offsets):\n",
    "    \"\"\"\n",
    "    Reads and parses binary data from a file into a DataFrame.\n",
    "    \n",
    "     - Number of measurement (size of the dump) found in 0-7 (unsigned long long) \n",
    "     - local0 found in 8-11 (float)\n",
    "     - local1 found in 12-15 (float)\n",
    "     - var_local0 found in offset 16-19 (float)\n",
    "     - var_local1 found in offset 20-23 (float)\n",
    "     - geometry ID found in offset 24-31 (unsigned long long)\n",
    "     - measurement_id found in offset 32-39 (unsigned long long)\n",
    "     - cluster_link found in offset 40-47 (unsigned long long)\n",
    "     - meas_dim found in offset 48-51 (unsigned int)\n",
    "\n",
    "    Args:\n",
    "        binary_file (str): Path to the binary file.\n",
    "        start_offset (int): Offset to start reading from.\n",
    "        ele_offsets (list): Byte offsets for unpacking.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the parsed data.\n",
    "    \"\"\"\n",
    "    # Define an empty DataFrame \n",
    "    data = []\n",
    "\n",
    "    # Get the total file size\n",
    "    file_size = os.path.getsize(binary_file)\n",
    "\n",
    "    # Read the binary data\n",
    "    with open(binary_file, \"rb\") as bin_file:\n",
    "        # mm = \n",
    "        print(\"number of measurements:\", struct.unpack(\"Q\", bin_file.read(8))[0])\n",
    "        \n",
    "        bin_file.seek(start_offset)\n",
    "        raw_bytes = bin_file.read(file_size - start_offset)\n",
    "\n",
    "\n",
    "    # Ensure offsets are consistent\n",
    "    if len(ele_offsets) < 9:\n",
    "        raise ValueError(\"ele_offsets must contain at least 9 elements for parsing.\")\n",
    "\n",
    "    # Process the binary data\n",
    "    for i in range(0, len(raw_bytes), ele_offsets[-1]):\n",
    "        try:\n",
    "            new_data = {\n",
    "                \"local0\": struct.unpack(\"f\", raw_bytes[i + ele_offsets[0]: i + ele_offsets[1]])[0],\n",
    "                \"local1\": struct.unpack(\"f\", raw_bytes[i + ele_offsets[1]: i + ele_offsets[2]])[0],\n",
    "                \"var_local0\": struct.unpack(\"f\", raw_bytes[i + ele_offsets[2]: i + ele_offsets[3]])[0],\n",
    "                \"var_local1\": struct.unpack(\"f\", raw_bytes[i + ele_offsets[3]: i + ele_offsets[4]])[0],\n",
    "                \"geometry_id\": struct.unpack(\"Q\", raw_bytes[i + ele_offsets[4]: i + ele_offsets[5]])[0],\n",
    "                \"measurement_id\": struct.unpack(\"Q\", raw_bytes[i + ele_offsets[5]: i + ele_offsets[6]])[0],\n",
    "                \"cluster_link\": struct.unpack(\"Q\", raw_bytes[i + ele_offsets[6]: i + ele_offsets[7]])[0],\n",
    "                \"meas_dim\": struct.unpack(\"I\", raw_bytes[i + ele_offsets[7]: i + ele_offsets[8]])[0],\n",
    "            }\n",
    "            data.append(new_data)\n",
    "        except struct.error as e:\n",
    "            print(f\"Error unpacking at index {i}: {e}\")\n",
    "            break\n",
    "    \n",
    "    # create the dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Configuration\n",
    "binary_file = \"test_measurements/event000000001-measurements.dat\"\n",
    "start_offset = 8\n",
    "ele_offsets = [0, 4, 8, 12, 16, 24, 32, 40, 44, 64]\n",
    "\n",
    "# Read the binary data into a DataFrame\n",
    "df = read_binary_data(binary_file, start_offset, ele_offsets)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file = \"test_measurements/measurements.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"{binary_file} saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992c8bc-42ee-4f7d-8fa9-96eb1b82185d",
   "metadata": {},
   "source": [
    "## Comparsion between dat2csv file and original csv file (within 3 decimal places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8aa665b-7c2f-4c25-b691-52db24404aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "253988it [00:01, 136239.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparsion between test_measurements/event000000001-measurements.csv and test_measurements/measurements.csv is completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Function to compare two floats within three decimal places\n",
    "def compare_within_three_decimals(val1, val2):\n",
    "    try:\n",
    "        return np.abs(float(val1) - float(val2)) < 1e-3\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Function to compare specific elements in CSV files\n",
    "# Limited to the first 10 rows\n",
    "\n",
    "def compare_specific_elements(file1, file2, element1, element2):\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        reader1 = csv.DictReader(f1)\n",
    "        reader2 = csv.DictReader(f2)\n",
    "\n",
    "        row_num = 1\n",
    "        for row1, row2 in tqdm(zip(reader1, reader2)):\n",
    "\n",
    "            local0_1 = row1.get(element1, None)\n",
    "            local1_1 = row1.get(element2, None)\n",
    "            local0_2 = row2.get(element1, None)\n",
    "            local1_2 = row2.get(element2, None)\n",
    "\n",
    "            if local0_1 is None or local0_2 is None or local1_1 is None or local1_2 is None:\n",
    "                print(f\"Missing element at row {row_num}: {element1} or {element2} not found.\")\n",
    "            elif not compare_within_three_decimals(local0_1, local0_2): # compare local0 value\n",
    "                print(f\"Mismatch at row {row_num}: {element1} = {local0_1}, {element1} = {local0_2}\")\n",
    "            elif not compare_within_three_decimals(local1_1, local1_2): # compare local1 value\n",
    "                print(f\"Mismatch at row {row_num}: {element2} = {local1_1}, {element2} = {local1_2}\")\n",
    "\n",
    "            row_num += 1\n",
    "        print(f\"Comparsion between {file1} and {file2} is completed\")\n",
    "\n",
    "compare_specific_elements('test_measurements/event000000001-measurements.csv', 'test_measurements/measurements.csv', 'local0', 'local1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d76a4-d8f6-4cc0-a76a-aa825180177d",
   "metadata": {},
   "source": [
    "## Create surface_link and geometry_id map (based on event000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7a3499-156d-4894-9a80-3818e995c853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_link to geometry_id map is saved to test_measurements/surface_link_geometry_map.csv\n"
     ]
    }
   ],
   "source": [
    "file1_path = 'test_measurements/event000000001-measurements.csv' # Traccc output\n",
    "file2_path = 'test_measurements/measurements.csv' # dat2csv output\n",
    "\n",
    "df1 = pd.read_csv(file1_path)\n",
    "df2 = pd.read_csv(file2_path)\n",
    "\n",
    "df1_selected = df1['surface_link']\n",
    "df2_selected = df2['geometry_id'] \n",
    "\n",
    "combined_df = pd.concat([df1_selected, df2_selected], axis=1)\n",
    "# Drop duplicate rows to ensure one-to-one mapping\n",
    "unique_map = combined_df.drop_duplicates()\n",
    "\n",
    "output_path = 'test_measurements/surface_link_geometry_map.csv'\n",
    "unique_map.to_csv(output_path, index=False)\n",
    "\n",
    "print(f'surface_link to geometry_id map is saved to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f84bc9-de0c-4a88-8051-b61e7bad1469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
